{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc3a33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "48250\n"
     ]
    }
   ],
   "source": [
    "# dataset for this model can be easily prepare by datasetmaker.py file\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "df = pd.read_csv('newresources.csv', index_col=0, engine = 'c')\n",
    "file = open(\"begining index of testing files.txt\",\"r\")\n",
    "data1 = int(file.read())\n",
    "file.close()\n",
    "row_num_for_verification_of_model = data1\n",
    "X = df.iloc[:row_num_for_verification_of_model,1:]  #independent variables columnns\n",
    "print(row_num_for_verification_of_model)\n",
    "X2 = df.iloc[row_num_for_verification_of_model:,1:]\n",
    "file = open(\"input dimension for model.txt\",\"r\")\n",
    "data2 = int(file.read())\n",
    "file.close()\n",
    "print(data2)\n",
    "total_number_of_column_required_for_prediction = data2\n",
    "column_number_of_csv_having_labels = 0\n",
    "y = df.iloc[:data1,column_number_of_csv_having_labels] # dependent variable column\n",
    "# # define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=total_number_of_column_required_for_prediction, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a100670c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C1E48CBAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C1E48CBAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C1E48CBAF0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - ETA: 0s - loss: 350.6739 - accuracy: 0.4947WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C1E41EBB80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C1E41EBB80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C1E41EBB80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 5s 1s/step - loss: 350.6739 - accuracy: 0.4947 - val_loss: 321.8418 - val_accuracy: 0.5208\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 735.9995 - accuracy: 0.5053 - val_loss: 399.3476 - val_accuracy: 0.5208\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 658.7452 - accuracy: 0.5158 - val_loss: 426.4671 - val_accuracy: 0.5208\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 467.0792 - accuracy: 0.5684 - val_loss: 435.9508 - val_accuracy: 0.5417\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 347.1072 - accuracy: 0.5895 - val_loss: 441.8947 - val_accuracy: 0.5417\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 287.6571 - accuracy: 0.5895 - val_loss: 443.9948 - val_accuracy: 0.5208\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 279.1295 - accuracy: 0.6105 - val_loss: 444.8848 - val_accuracy: 0.5417\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 230.3615 - accuracy: 0.6211 - val_loss: 441.5091 - val_accuracy: 0.5417\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 201.7870 - accuracy: 0.6211 - val_loss: 431.1913 - val_accuracy: 0.5417\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 173.0847 - accuracy: 0.6316 - val_loss: 425.2655 - val_accuracy: 0.5417\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 129.5643 - accuracy: 0.6421 - val_loss: 423.4392 - val_accuracy: 0.5417\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 133.4337 - accuracy: 0.6526 - val_loss: 420.9264 - val_accuracy: 0.5417\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 138.9913 - accuracy: 0.6526 - val_loss: 412.5520 - val_accuracy: 0.5417\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 101.9406 - accuracy: 0.6632 - val_loss: 403.4605 - val_accuracy: 0.5417\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 72.0116 - accuracy: 0.6842 - val_loss: 397.6354 - val_accuracy: 0.5417\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 68.1117 - accuracy: 0.6947 - val_loss: 388.7725 - val_accuracy: 0.5417\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 51.3720 - accuracy: 0.7368 - val_loss: 381.4431 - val_accuracy: 0.5417\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 51.1188 - accuracy: 0.7895 - val_loss: 376.1993 - val_accuracy: 0.5417\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 48.3427 - accuracy: 0.8105 - val_loss: 371.2323 - val_accuracy: 0.5417\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 43.1577 - accuracy: 0.8421 - val_loss: 367.5479 - val_accuracy: 0.5417\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 35.4211 - accuracy: 0.8421 - val_loss: 365.1571 - val_accuracy: 0.5417\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 48.1526 - accuracy: 0.8421 - val_loss: 364.2206 - val_accuracy: 0.5417\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 39.8066 - accuracy: 0.8526 - val_loss: 362.2686 - val_accuracy: 0.5417\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 24.7180 - accuracy: 0.8632 - val_loss: 360.2167 - val_accuracy: 0.5417\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 23.2838 - accuracy: 0.8526 - val_loss: 358.4441 - val_accuracy: 0.5417\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 26.2246 - accuracy: 0.8737 - val_loss: 356.4072 - val_accuracy: 0.5417\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 29.8826 - accuracy: 0.8737 - val_loss: 352.8021 - val_accuracy: 0.5417\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 23.1674 - accuracy: 0.8737 - val_loss: 348.4592 - val_accuracy: 0.5417\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 201ms/step - loss: 18.0293 - accuracy: 0.8737 - val_loss: 344.2288 - val_accuracy: 0.5417\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 18.5413 - accuracy: 0.8632 - val_loss: 341.0609 - val_accuracy: 0.5417\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 22.7282 - accuracy: 0.8737 - val_loss: 339.0592 - val_accuracy: 0.5208\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 21.5223 - accuracy: 0.9053 - val_loss: 335.5804 - val_accuracy: 0.5625\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 20.2298 - accuracy: 0.9053 - val_loss: 333.0073 - val_accuracy: 0.5625\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 19.7573 - accuracy: 0.9158 - val_loss: 331.4273 - val_accuracy: 0.5625\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 19.4465 - accuracy: 0.9053 - val_loss: 328.4064 - val_accuracy: 0.5417\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 17.2333 - accuracy: 0.9158 - val_loss: 325.0588 - val_accuracy: 0.5417\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 16.7169 - accuracy: 0.9053 - val_loss: 323.3335 - val_accuracy: 0.5417\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 18.4306 - accuracy: 0.9053 - val_loss: 324.9737 - val_accuracy: 0.5208\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 14.1414 - accuracy: 0.9053 - val_loss: 325.0013 - val_accuracy: 0.5208\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 19.9457 - accuracy: 0.9158 - val_loss: 323.6862 - val_accuracy: 0.5208\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 19.5441 - accuracy: 0.9053 - val_loss: 321.3137 - val_accuracy: 0.5208\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 18.5420 - accuracy: 0.9053 - val_loss: 319.9605 - val_accuracy: 0.5208\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 17.7391 - accuracy: 0.9158 - val_loss: 318.3891 - val_accuracy: 0.5208\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 17.3243 - accuracy: 0.9368 - val_loss: 317.7198 - val_accuracy: 0.5208\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 25.5046 - accuracy: 0.9263 - val_loss: 316.0244 - val_accuracy: 0.5208\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 25.5620 - accuracy: 0.9263 - val_loss: 314.0109 - val_accuracy: 0.5208\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 23.2520 - accuracy: 0.9158 - val_loss: 312.5291 - val_accuracy: 0.5208\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 26.9699 - accuracy: 0.9368 - val_loss: 310.0095 - val_accuracy: 0.5208\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 21.9219 - accuracy: 0.9368 - val_loss: 308.0074 - val_accuracy: 0.5208\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 21.8328 - accuracy: 0.9368 - val_loss: 306.0237 - val_accuracy: 0.5208\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 22.6830 - accuracy: 0.9368 - val_loss: 304.6397 - val_accuracy: 0.5208\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 19.7401 - accuracy: 0.9368 - val_loss: 303.5393 - val_accuracy: 0.5208\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 17.6999 - accuracy: 0.9368 - val_loss: 302.8791 - val_accuracy: 0.5208\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 142ms/step - loss: 17.0242 - accuracy: 0.9368 - val_loss: 301.3371 - val_accuracy: 0.5208\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 16.3249 - accuracy: 0.9368 - val_loss: 300.3633 - val_accuracy: 0.5208\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 12.6507 - accuracy: 0.9368 - val_loss: 299.6345 - val_accuracy: 0.5208\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 6.9533 - accuracy: 0.9368 - val_loss: 297.1216 - val_accuracy: 0.5208\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 5.2627 - accuracy: 0.9368 - val_loss: 294.9972 - val_accuracy: 0.5208\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 9.6201 - accuracy: 0.9368 - val_loss: 293.2805 - val_accuracy: 0.5208\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 5.0034 - accuracy: 0.9579 - val_loss: 291.8799 - val_accuracy: 0.5208\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 7.6753 - accuracy: 0.9579 - val_loss: 291.1986 - val_accuracy: 0.5208\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 7.3356 - accuracy: 0.9579 - val_loss: 290.8800 - val_accuracy: 0.5208\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 7.1196 - accuracy: 0.9579 - val_loss: 289.4348 - val_accuracy: 0.5208\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 5.6492 - accuracy: 0.9684 - val_loss: 287.9555 - val_accuracy: 0.5208\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 7.1062 - accuracy: 0.9474 - val_loss: 286.8502 - val_accuracy: 0.5208\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 2.8848 - accuracy: 0.9474 - val_loss: 286.2649 - val_accuracy: 0.5208\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 3.5028 - accuracy: 0.9684 - val_loss: 285.9833 - val_accuracy: 0.5208\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 3.8017 - accuracy: 0.9684 - val_loss: 285.7866 - val_accuracy: 0.5208\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 3.8700 - accuracy: 0.9789 - val_loss: 285.3341 - val_accuracy: 0.5208\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 3.7620 - accuracy: 0.9789 - val_loss: 284.8308 - val_accuracy: 0.5208\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 5.8533 - accuracy: 0.9684 - val_loss: 284.6161 - val_accuracy: 0.5208\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 7.0246 - accuracy: 0.9684 - val_loss: 284.4399 - val_accuracy: 0.5208\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 6.9307 - accuracy: 0.9789 - val_loss: 283.9560 - val_accuracy: 0.5208\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 7.5999 - accuracy: 0.9684 - val_loss: 283.2556 - val_accuracy: 0.5208\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 6.9701 - accuracy: 0.9684 - val_loss: 282.5833 - val_accuracy: 0.5208\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 6.4556 - accuracy: 0.9684 - val_loss: 282.6165 - val_accuracy: 0.5208\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 6.1787 - accuracy: 0.9684 - val_loss: 282.9929 - val_accuracy: 0.5208\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 5.9152 - accuracy: 0.9684 - val_loss: 283.3081 - val_accuracy: 0.5208\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 4.7253 - accuracy: 0.9684 - val_loss: 283.7453 - val_accuracy: 0.5208\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 4.9778 - accuracy: 0.9684 - val_loss: 283.8543 - val_accuracy: 0.5208\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 5.0074 - accuracy: 0.9684 - val_loss: 283.5219 - val_accuracy: 0.5208\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 3.9947 - accuracy: 0.9684 - val_loss: 283.2415 - val_accuracy: 0.5208\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 3.2278 - accuracy: 0.9684 - val_loss: 283.1334 - val_accuracy: 0.5208\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 3.0118 - accuracy: 0.9684 - val_loss: 283.0069 - val_accuracy: 0.5208\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 7.5315 - accuracy: 0.9684 - val_loss: 282.1545 - val_accuracy: 0.5208\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 5.0421 - accuracy: 0.9684 - val_loss: 280.5163 - val_accuracy: 0.5208\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 5.6583 - accuracy: 0.9684 - val_loss: 279.3385 - val_accuracy: 0.5208\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 6.8903 - accuracy: 0.9684 - val_loss: 277.1330 - val_accuracy: 0.5208\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 7.0443 - accuracy: 0.9789 - val_loss: 275.9719 - val_accuracy: 0.5208\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 7.4178 - accuracy: 0.9789 - val_loss: 275.4482 - val_accuracy: 0.5208\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 7.4513 - accuracy: 0.9789 - val_loss: 275.7833 - val_accuracy: 0.5208\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 7.3862 - accuracy: 0.9789 - val_loss: 276.1621 - val_accuracy: 0.5208\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.1086 - accuracy: 0.9684 - val_loss: 277.2493 - val_accuracy: 0.5208\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 7.5267 - accuracy: 0.9684 - val_loss: 277.9960 - val_accuracy: 0.5208\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 6.0630 - accuracy: 0.9789 - val_loss: 278.1170 - val_accuracy: 0.5208\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 5.7828 - accuracy: 0.9789 - val_loss: 277.9853 - val_accuracy: 0.5208\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 4.9920 - accuracy: 0.9789 - val_loss: 277.4918 - val_accuracy: 0.5208\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 4.2972 - accuracy: 0.9789 - val_loss: 277.1895 - val_accuracy: 0.5208\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 5.9521 - accuracy: 0.9789 - val_loss: 276.7937 - val_accuracy: 0.5208\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 4.7523 - accuracy: 0.9789 - val_loss: 276.3296 - val_accuracy: 0.5208\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 4.1274 - accuracy: 0.9789 - val_loss: 275.8419 - val_accuracy: 0.5208\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 4.7358 - accuracy: 0.9789 - val_loss: 275.3387 - val_accuracy: 0.5208\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 3.9598 - accuracy: 0.9789 - val_loss: 274.6666 - val_accuracy: 0.5208\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 2.2356 - accuracy: 0.9895 - val_loss: 274.4276 - val_accuracy: 0.5417\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 1.9093 - accuracy: 0.9895 - val_loss: 276.2553 - val_accuracy: 0.5417\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 2.7244 - accuracy: 0.9789 - val_loss: 278.8675 - val_accuracy: 0.5417\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 2.3563 - accuracy: 0.9895 - val_loss: 279.6166 - val_accuracy: 0.5417\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 2.1584 - accuracy: 0.9789 - val_loss: 279.3944 - val_accuracy: 0.5417\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 2.8878 - accuracy: 0.9895 - val_loss: 278.3109 - val_accuracy: 0.5417\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 2.8069 - accuracy: 0.9895 - val_loss: 276.9366 - val_accuracy: 0.5417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 2.5211 - accuracy: 0.9895 - val_loss: 275.3953 - val_accuracy: 0.5417\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 2.5422 - accuracy: 0.9789 - val_loss: 274.5940 - val_accuracy: 0.5417\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 2.7581 - accuracy: 0.9895 - val_loss: 273.5888 - val_accuracy: 0.5417\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 2.9694 - accuracy: 0.9789 - val_loss: 271.9949 - val_accuracy: 0.5417\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 2.9066 - accuracy: 0.9895 - val_loss: 271.5817 - val_accuracy: 0.5417\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 2.7032 - accuracy: 0.9895 - val_loss: 271.4075 - val_accuracy: 0.5208\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 3.1072 - accuracy: 0.9895 - val_loss: 270.8906 - val_accuracy: 0.5208\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 2.5607 - accuracy: 0.9895 - val_loss: 270.4354 - val_accuracy: 0.5208\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 2.7462 - accuracy: 0.9895 - val_loss: 270.1754 - val_accuracy: 0.5208\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 2.4922 - accuracy: 0.9895 - val_loss: 269.9948 - val_accuracy: 0.5208\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 2.1869 - accuracy: 0.9895 - val_loss: 270.0901 - val_accuracy: 0.5208\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 3.6878 - accuracy: 0.9895 - val_loss: 270.3384 - val_accuracy: 0.5208\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.6377 - accuracy: 0.9895 - val_loss: 270.3266 - val_accuracy: 0.5208\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 3.3009 - accuracy: 0.9895 - val_loss: 270.0180 - val_accuracy: 0.5208\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 2.3180 - accuracy: 0.9895 - val_loss: 269.8637 - val_accuracy: 0.5208\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 2.2847 - accuracy: 0.9895 - val_loss: 269.9872 - val_accuracy: 0.5208\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 1.8624 - accuracy: 0.9895 - val_loss: 269.8257 - val_accuracy: 0.5208\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 2.4303 - accuracy: 0.9895 - val_loss: 269.5498 - val_accuracy: 0.5208\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.9940 - accuracy: 0.9895 - val_loss: 269.4474 - val_accuracy: 0.5208\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 1.8138 - accuracy: 0.9895 - val_loss: 269.6115 - val_accuracy: 0.5208\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 1.4503 - accuracy: 0.9895 - val_loss: 269.5670 - val_accuracy: 0.5208\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 2.1557 - accuracy: 0.9895 - val_loss: 268.9537 - val_accuracy: 0.5208\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.9268 - accuracy: 0.9895 - val_loss: 268.3577 - val_accuracy: 0.5208\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 1.3435 - accuracy: 0.9895 - val_loss: 268.0109 - val_accuracy: 0.5208\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 1.2832 - accuracy: 0.9895 - val_loss: 267.8313 - val_accuracy: 0.5208\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 1.0441 - accuracy: 0.9895 - val_loss: 267.9853 - val_accuracy: 0.5208\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 1.7397 - accuracy: 0.9895 - val_loss: 267.7085 - val_accuracy: 0.5208\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 1.3154 - accuracy: 0.9895 - val_loss: 267.3028 - val_accuracy: 0.5208\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 1.6707 - accuracy: 0.9895 - val_loss: 267.1958 - val_accuracy: 0.5208\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 1.2910 - accuracy: 0.9895 - val_loss: 267.5167 - val_accuracy: 0.5208\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 2.6550 - accuracy: 0.9895 - val_loss: 266.6267 - val_accuracy: 0.5208\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 1.6653 - accuracy: 0.9895 - val_loss: 265.6062 - val_accuracy: 0.5208\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 2.0530 - accuracy: 0.9895 - val_loss: 264.9724 - val_accuracy: 0.5000\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 2.5917 - accuracy: 0.9895 - val_loss: 264.5009 - val_accuracy: 0.5000\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 2.7306 - accuracy: 0.9895 - val_loss: 264.1707 - val_accuracy: 0.5000\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 2.5813 - accuracy: 0.9895 - val_loss: 263.9557 - val_accuracy: 0.5000\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 2.3264 - accuracy: 0.9895 - val_loss: 263.8574 - val_accuracy: 0.5000\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 1.5827 - accuracy: 0.9895 - val_loss: 263.8208 - val_accuracy: 0.5208\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.8382 - accuracy: 0.9895 - val_loss: 262.8758 - val_accuracy: 0.5000\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 4.2260 - accuracy: 0.9895 - val_loss: 262.2740 - val_accuracy: 0.5000\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 7.1367 - accuracy: 0.9789 - val_loss: 261.6208 - val_accuracy: 0.5000\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 7.9305 - accuracy: 0.9895 - val_loss: 261.1503 - val_accuracy: 0.5000\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 8.2585 - accuracy: 0.9895 - val_loss: 260.3951 - val_accuracy: 0.5000\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 8.8998 - accuracy: 0.9895 - val_loss: 259.6690 - val_accuracy: 0.5000\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 9.1025 - accuracy: 0.9895 - val_loss: 258.9632 - val_accuracy: 0.5000\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 9.1245 - accuracy: 0.9895 - val_loss: 258.3325 - val_accuracy: 0.5000\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 9.0250 - accuracy: 0.9895 - val_loss: 257.8112 - val_accuracy: 0.5000\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 8.9244 - accuracy: 0.9895 - val_loss: 257.3325 - val_accuracy: 0.5000\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 8.6673 - accuracy: 0.9895 - val_loss: 256.8828 - val_accuracy: 0.5000\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 8.2641 - accuracy: 0.9895 - val_loss: 256.4789 - val_accuracy: 0.5000\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 7.9787 - accuracy: 0.9895 - val_loss: 256.0947 - val_accuracy: 0.5000\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 7.8160 - accuracy: 0.9895 - val_loss: 255.7382 - val_accuracy: 0.5000\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 7.3270 - accuracy: 0.9895 - val_loss: 255.3917 - val_accuracy: 0.5000\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 7.1588 - accuracy: 0.9895 - val_loss: 255.0064 - val_accuracy: 0.5000\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 6.6652 - accuracy: 0.9895 - val_loss: 254.6741 - val_accuracy: 0.5000\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 6.4991 - accuracy: 0.9895 - val_loss: 254.3288 - val_accuracy: 0.5000\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 131ms/step - loss: 6.1603 - accuracy: 0.9895 - val_loss: 253.9915 - val_accuracy: 0.5000\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 5.8346 - accuracy: 0.9895 - val_loss: 253.7223 - val_accuracy: 0.5000\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 5.5216 - accuracy: 0.9895 - val_loss: 253.4805 - val_accuracy: 0.5000\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 5.0945 - accuracy: 0.9895 - val_loss: 253.5634 - val_accuracy: 0.5000\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 4.8260 - accuracy: 0.9895 - val_loss: 253.7832 - val_accuracy: 0.5000\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 4.6849 - accuracy: 0.9895 - val_loss: 254.0275 - val_accuracy: 0.5208\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 4.2809 - accuracy: 0.9895 - val_loss: 254.2846 - val_accuracy: 0.5208\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 4.1479 - accuracy: 0.9895 - val_loss: 254.3294 - val_accuracy: 0.5208\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 3.8801 - accuracy: 0.9895 - val_loss: 254.1844 - val_accuracy: 0.5208\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 3.6247 - accuracy: 0.9895 - val_loss: 254.0542 - val_accuracy: 0.5208\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 3.2776 - accuracy: 0.9895 - val_loss: 253.8040 - val_accuracy: 0.5208\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 3.1625 - accuracy: 0.9895 - val_loss: 253.5411 - val_accuracy: 0.5208\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 2.8314 - accuracy: 0.9895 - val_loss: 253.2816 - val_accuracy: 0.5208\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 2.7216 - accuracy: 0.9895 - val_loss: 252.9899 - val_accuracy: 0.5208\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 2.4994 - accuracy: 0.9895 - val_loss: 252.7276 - val_accuracy: 0.5208\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 2.1958 - accuracy: 0.9895 - val_loss: 252.5399 - val_accuracy: 0.5208\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 2.0945 - accuracy: 0.9895 - val_loss: 252.3518 - val_accuracy: 0.5208\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.8018 - accuracy: 0.9895 - val_loss: 251.9897 - val_accuracy: 0.5208\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 1.6166 - accuracy: 0.9895 - val_loss: 251.5766 - val_accuracy: 0.5208\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 1.5188 - accuracy: 0.9895 - val_loss: 251.3172 - val_accuracy: 0.5208\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 1.3201 - accuracy: 0.9895 - val_loss: 251.1983 - val_accuracy: 0.5208\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 1.0468 - accuracy: 0.9895 - val_loss: 251.0981 - val_accuracy: 0.5208\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.8727 - accuracy: 0.9895 - val_loss: 251.0084 - val_accuracy: 0.5208\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7802 - accuracy: 0.9895 - val_loss: 250.9968 - val_accuracy: 0.5417\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.5113 - accuracy: 0.9895 - val_loss: 251.1065 - val_accuracy: 0.5417\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.3398 - accuracy: 0.9895 - val_loss: 251.2308 - val_accuracy: 0.5417\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.2772 - accuracy: 1.0000 - val_loss: 251.3485 - val_accuracy: 0.5417\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2769 - accuracy: 1.0000 - val_loss: 251.4443 - val_accuracy: 0.5417\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2766 - accuracy: 1.0000 - val_loss: 251.5224 - val_accuracy: 0.5417\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2762 - accuracy: 1.0000 - val_loss: 251.5860 - val_accuracy: 0.5417\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2759 - accuracy: 1.0000 - val_loss: 251.6376 - val_accuracy: 0.5417\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2755 - accuracy: 1.0000 - val_loss: 251.6795 - val_accuracy: 0.5417\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2752 - accuracy: 1.0000 - val_loss: 251.7136 - val_accuracy: 0.5417\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2748 - accuracy: 1.0000 - val_loss: 251.7412 - val_accuracy: 0.5417\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2745 - accuracy: 1.0000 - val_loss: 251.7635 - val_accuracy: 0.5417\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2742 - accuracy: 1.0000 - val_loss: 251.7816 - val_accuracy: 0.5417\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2738 - accuracy: 1.0000 - val_loss: 251.7961 - val_accuracy: 0.5417\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.2735 - accuracy: 1.0000 - val_loss: 251.8078 - val_accuracy: 0.5417\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.2731 - accuracy: 1.0000 - val_loss: 251.8172 - val_accuracy: 0.5417\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2728 - accuracy: 1.0000 - val_loss: 251.8248 - val_accuracy: 0.5417\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2725 - accuracy: 1.0000 - val_loss: 251.8306 - val_accuracy: 0.5417\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2721 - accuracy: 1.0000 - val_loss: 251.8354 - val_accuracy: 0.5417\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2718 - accuracy: 1.0000 - val_loss: 251.8392 - val_accuracy: 0.5417\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2715 - accuracy: 1.0000 - val_loss: 251.8421 - val_accuracy: 0.5625\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2711 - accuracy: 1.0000 - val_loss: 251.8443 - val_accuracy: 0.5625\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2708 - accuracy: 1.0000 - val_loss: 251.8459 - val_accuracy: 0.5625\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2705 - accuracy: 1.0000 - val_loss: 251.8472 - val_accuracy: 0.5625\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2701 - accuracy: 1.0000 - val_loss: 251.8481 - val_accuracy: 0.5625\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2698 - accuracy: 1.0000 - val_loss: 251.8487 - val_accuracy: 0.5625\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2695 - accuracy: 1.0000 - val_loss: 251.8490 - val_accuracy: 0.5625\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2692 - accuracy: 1.0000 - val_loss: 251.8492 - val_accuracy: 0.5625\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2688 - accuracy: 1.0000 - val_loss: 251.8492 - val_accuracy: 0.5625\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2685 - accuracy: 1.0000 - val_loss: 251.8490 - val_accuracy: 0.5625\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2681 - accuracy: 1.0000 - val_loss: 251.8488 - val_accuracy: 0.5625\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2678 - accuracy: 1.0000 - val_loss: 251.8485 - val_accuracy: 0.5625\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2675 - accuracy: 1.0000 - val_loss: 251.8481 - val_accuracy: 0.5625\n",
      "Epoch 223/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2671 - accuracy: 1.0000 - val_loss: 251.8477 - val_accuracy: 0.5625\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2668 - accuracy: 1.0000 - val_loss: 251.8472 - val_accuracy: 0.5625\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2665 - accuracy: 1.0000 - val_loss: 251.8467 - val_accuracy: 0.5625\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2662 - accuracy: 1.0000 - val_loss: 251.8462 - val_accuracy: 0.5625\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2658 - accuracy: 1.0000 - val_loss: 251.8456 - val_accuracy: 0.5625\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2655 - accuracy: 1.0000 - val_loss: 251.8450 - val_accuracy: 0.5625\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2652 - accuracy: 1.0000 - val_loss: 251.8444 - val_accuracy: 0.5625\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2649 - accuracy: 1.0000 - val_loss: 251.8438 - val_accuracy: 0.5625\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2645 - accuracy: 1.0000 - val_loss: 251.8431 - val_accuracy: 0.5625\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2642 - accuracy: 1.0000 - val_loss: 251.8425 - val_accuracy: 0.5625\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2639 - accuracy: 1.0000 - val_loss: 251.8419 - val_accuracy: 0.5625\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2635 - accuracy: 1.0000 - val_loss: 251.8412 - val_accuracy: 0.5625\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2632 - accuracy: 1.0000 - val_loss: 251.8406 - val_accuracy: 0.5625\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2629 - accuracy: 1.0000 - val_loss: 251.8399 - val_accuracy: 0.5625\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2626 - accuracy: 1.0000 - val_loss: 251.8392 - val_accuracy: 0.5625\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2622 - accuracy: 1.0000 - val_loss: 251.8386 - val_accuracy: 0.5625\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2619 - accuracy: 1.0000 - val_loss: 251.8379 - val_accuracy: 0.5625\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2616 - accuracy: 1.0000 - val_loss: 251.8372 - val_accuracy: 0.5625\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2613 - accuracy: 1.0000 - val_loss: 251.8366 - val_accuracy: 0.5625\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2609 - accuracy: 1.0000 - val_loss: 251.8359 - val_accuracy: 0.5625\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2606 - accuracy: 1.0000 - val_loss: 251.8352 - val_accuracy: 0.5625\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2603 - accuracy: 1.0000 - val_loss: 251.8345 - val_accuracy: 0.5625\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2600 - accuracy: 1.0000 - val_loss: 251.8339 - val_accuracy: 0.5625\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.2596 - accuracy: 1.0000 - val_loss: 251.8332 - val_accuracy: 0.5625\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2593 - accuracy: 1.0000 - val_loss: 251.8325 - val_accuracy: 0.5625\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.2590 - accuracy: 1.0000 - val_loss: 251.8318 - val_accuracy: 0.5625\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.2587 - accuracy: 1.0000 - val_loss: 251.8311 - val_accuracy: 0.5625\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.2583 - accuracy: 1.0000 - val_loss: 251.8305 - val_accuracy: 0.5625\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2580 - accuracy: 1.0000 - val_loss: 251.8298 - val_accuracy: 0.5625\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2577 - accuracy: 1.0000 - val_loss: 251.8291 - val_accuracy: 0.5625\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2574 - accuracy: 1.0000 - val_loss: 251.8284 - val_accuracy: 0.5625\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.2571 - accuracy: 1.0000 - val_loss: 251.8278 - val_accuracy: 0.5625\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.2567 - accuracy: 1.0000 - val_loss: 251.8271 - val_accuracy: 0.5625\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.2564 - accuracy: 1.0000 - val_loss: 251.8264 - val_accuracy: 0.5625\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2561 - accuracy: 1.0000 - val_loss: 251.8258 - val_accuracy: 0.5625\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2558 - accuracy: 1.0000 - val_loss: 251.8251 - val_accuracy: 0.5625\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2555 - accuracy: 1.0000 - val_loss: 251.8244 - val_accuracy: 0.5625\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2551 - accuracy: 1.0000 - val_loss: 251.8237 - val_accuracy: 0.5625\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2548 - accuracy: 1.0000 - val_loss: 251.8230 - val_accuracy: 0.5625\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2545 - accuracy: 1.0000 - val_loss: 251.8224 - val_accuracy: 0.5625\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2542 - accuracy: 1.0000 - val_loss: 251.8217 - val_accuracy: 0.5625\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2539 - accuracy: 1.0000 - val_loss: 251.8210 - val_accuracy: 0.5625\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2535 - accuracy: 1.0000 - val_loss: 251.8204 - val_accuracy: 0.5625\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.2532 - accuracy: 1.0000 - val_loss: 251.8197 - val_accuracy: 0.5625\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2529 - accuracy: 1.0000 - val_loss: 251.8190 - val_accuracy: 0.5625\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2526 - accuracy: 1.0000 - val_loss: 251.8183 - val_accuracy: 0.5625\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2523 - accuracy: 1.0000 - val_loss: 251.8176 - val_accuracy: 0.5625\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2520 - accuracy: 1.0000 - val_loss: 251.8170 - val_accuracy: 0.5625\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2516 - accuracy: 1.0000 - val_loss: 251.8163 - val_accuracy: 0.5625\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2513 - accuracy: 1.0000 - val_loss: 251.8156 - val_accuracy: 0.5625\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2510 - accuracy: 1.0000 - val_loss: 251.8150 - val_accuracy: 0.5625\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.2507 - accuracy: 1.0000 - val_loss: 251.8143 - val_accuracy: 0.5625\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 251.8136 - val_accuracy: 0.5625\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2501 - accuracy: 1.0000 - val_loss: 251.8129 - val_accuracy: 0.5625\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2498 - accuracy: 1.0000 - val_loss: 251.8123 - val_accuracy: 0.5625\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2494 - accuracy: 1.0000 - val_loss: 251.8116 - val_accuracy: 0.5625\n",
      "Epoch 279/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2491 - accuracy: 1.0000 - val_loss: 251.8109 - val_accuracy: 0.5625\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 0.2488 - accuracy: 1.0000 - val_loss: 251.8102 - val_accuracy: 0.5625\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2485 - accuracy: 1.0000 - val_loss: 251.8096 - val_accuracy: 0.5625\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2482 - accuracy: 1.0000 - val_loss: 251.8089 - val_accuracy: 0.5625\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2479 - accuracy: 1.0000 - val_loss: 251.8082 - val_accuracy: 0.5625\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2476 - accuracy: 1.0000 - val_loss: 251.8075 - val_accuracy: 0.5625\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2473 - accuracy: 1.0000 - val_loss: 251.8069 - val_accuracy: 0.5625\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2469 - accuracy: 1.0000 - val_loss: 251.8062 - val_accuracy: 0.5625\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2466 - accuracy: 1.0000 - val_loss: 251.8055 - val_accuracy: 0.5625\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 251.8049 - val_accuracy: 0.5625\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2460 - accuracy: 1.0000 - val_loss: 251.8042 - val_accuracy: 0.5625\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.2457 - accuracy: 1.0000 - val_loss: 251.8035 - val_accuracy: 0.5625\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.2454 - accuracy: 1.0000 - val_loss: 251.8028 - val_accuracy: 0.5625\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2451 - accuracy: 1.0000 - val_loss: 251.8021 - val_accuracy: 0.5625\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2448 - accuracy: 1.0000 - val_loss: 251.8015 - val_accuracy: 0.5625\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2445 - accuracy: 1.0000 - val_loss: 251.8008 - val_accuracy: 0.5625\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2442 - accuracy: 1.0000 - val_loss: 251.8001 - val_accuracy: 0.5625\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2438 - accuracy: 1.0000 - val_loss: 251.7995 - val_accuracy: 0.5625\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2435 - accuracy: 1.0000 - val_loss: 251.7988 - val_accuracy: 0.5625\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2432 - accuracy: 1.0000 - val_loss: 251.7981 - val_accuracy: 0.5625\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2429 - accuracy: 1.0000 - val_loss: 251.7974 - val_accuracy: 0.5625\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2426 - accuracy: 1.0000 - val_loss: 251.7968 - val_accuracy: 0.5625\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.2423 - accuracy: 1.0000 - val_loss: 251.7961 - val_accuracy: 0.5625\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2420 - accuracy: 1.0000 - val_loss: 251.7954 - val_accuracy: 0.5625\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2417 - accuracy: 1.0000 - val_loss: 251.7948 - val_accuracy: 0.5625\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.2414 - accuracy: 1.0000 - val_loss: 251.7941 - val_accuracy: 0.5625\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2411 - accuracy: 1.0000 - val_loss: 251.7935 - val_accuracy: 0.5625\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2408 - accuracy: 1.0000 - val_loss: 251.7927 - val_accuracy: 0.5625\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2405 - accuracy: 1.0000 - val_loss: 251.7921 - val_accuracy: 0.5625\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.2402 - accuracy: 1.0000 - val_loss: 251.7914 - val_accuracy: 0.5625\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.2399 - accuracy: 1.0000 - val_loss: 251.7907 - val_accuracy: 0.5625\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.2396 - accuracy: 1.0000 - val_loss: 251.7901 - val_accuracy: 0.5625\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2393 - accuracy: 1.0000 - val_loss: 251.7894 - val_accuracy: 0.5625\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2390 - accuracy: 1.0000 - val_loss: 251.7887 - val_accuracy: 0.5625\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2387 - accuracy: 1.0000 - val_loss: 251.7880 - val_accuracy: 0.5625\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.2384 - accuracy: 1.0000 - val_loss: 251.7874 - val_accuracy: 0.5625\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2381 - accuracy: 1.0000 - val_loss: 251.7867 - val_accuracy: 0.5625\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.2378 - accuracy: 1.0000 - val_loss: 251.7861 - val_accuracy: 0.5625\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2375 - accuracy: 1.0000 - val_loss: 251.7854 - val_accuracy: 0.5625\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2372 - accuracy: 1.0000 - val_loss: 251.7847 - val_accuracy: 0.5625\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2369 - accuracy: 1.0000 - val_loss: 251.7840 - val_accuracy: 0.5625\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2366 - accuracy: 1.0000 - val_loss: 251.7834 - val_accuracy: 0.5625\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.2363 - accuracy: 1.0000 - val_loss: 251.7827 - val_accuracy: 0.5625\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2360 - accuracy: 1.0000 - val_loss: 251.7821 - val_accuracy: 0.5625\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2357 - accuracy: 1.0000 - val_loss: 251.7814 - val_accuracy: 0.5625\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 251.7807 - val_accuracy: 0.5625\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2351 - accuracy: 1.0000 - val_loss: 251.7800 - val_accuracy: 0.5625\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2348 - accuracy: 1.0000 - val_loss: 251.7794 - val_accuracy: 0.5625\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2345 - accuracy: 1.0000 - val_loss: 251.7787 - val_accuracy: 0.5625\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2342 - accuracy: 1.0000 - val_loss: 251.7780 - val_accuracy: 0.5625\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.2339 - accuracy: 1.0000 - val_loss: 251.7774 - val_accuracy: 0.5625\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2336 - accuracy: 1.0000 - val_loss: 251.7767 - val_accuracy: 0.5625\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2333 - accuracy: 1.0000 - val_loss: 251.7760 - val_accuracy: 0.5625\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2330 - accuracy: 1.0000 - val_loss: 251.7754 - val_accuracy: 0.5625\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.2327 - accuracy: 1.0000 - val_loss: 251.7747 - val_accuracy: 0.5625\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.2324 - accuracy: 1.0000 - val_loss: 251.7741 - val_accuracy: 0.5625\n",
      "Epoch 335/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2321 - accuracy: 1.0000 - val_loss: 251.7734 - val_accuracy: 0.5625\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2318 - accuracy: 1.0000 - val_loss: 251.7728 - val_accuracy: 0.5625\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2316 - accuracy: 1.0000 - val_loss: 251.7720 - val_accuracy: 0.5625\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2313 - accuracy: 1.0000 - val_loss: 251.7714 - val_accuracy: 0.5625\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2309 - accuracy: 1.0000 - val_loss: 251.7708 - val_accuracy: 0.5625\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2307 - accuracy: 1.0000 - val_loss: 251.7701 - val_accuracy: 0.5625\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2304 - accuracy: 1.0000 - val_loss: 251.7694 - val_accuracy: 0.5625\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2301 - accuracy: 1.0000 - val_loss: 251.7688 - val_accuracy: 0.5625\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2298 - accuracy: 1.0000 - val_loss: 251.7681 - val_accuracy: 0.5625\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.2295 - accuracy: 1.0000 - val_loss: 251.7674 - val_accuracy: 0.5625\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2292 - accuracy: 1.0000 - val_loss: 251.7668 - val_accuracy: 0.5625\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2289 - accuracy: 1.0000 - val_loss: 251.7661 - val_accuracy: 0.5625\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2286 - accuracy: 1.0000 - val_loss: 251.7654 - val_accuracy: 0.5625\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2283 - accuracy: 1.0000 - val_loss: 251.7648 - val_accuracy: 0.5625\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2281 - accuracy: 1.0000 - val_loss: 251.7641 - val_accuracy: 0.5625\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2278 - accuracy: 1.0000 - val_loss: 251.7634 - val_accuracy: 0.5625\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2275 - accuracy: 1.0000 - val_loss: 251.7628 - val_accuracy: 0.5625\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2272 - accuracy: 1.0000 - val_loss: 251.7621 - val_accuracy: 0.5625\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2269 - accuracy: 1.0000 - val_loss: 251.7615 - val_accuracy: 0.5625\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2266 - accuracy: 1.0000 - val_loss: 251.7608 - val_accuracy: 0.5625\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2263 - accuracy: 1.0000 - val_loss: 251.7601 - val_accuracy: 0.5625\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2260 - accuracy: 1.0000 - val_loss: 251.7595 - val_accuracy: 0.5625\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.2258 - accuracy: 1.0000 - val_loss: 251.7588 - val_accuracy: 0.5625\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.2255 - accuracy: 1.0000 - val_loss: 251.7582 - val_accuracy: 0.5625\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.2252 - accuracy: 1.0000 - val_loss: 251.7575 - val_accuracy: 0.5625\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2249 - accuracy: 1.0000 - val_loss: 251.7568 - val_accuracy: 0.5625\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2246 - accuracy: 1.0000 - val_loss: 251.7562 - val_accuracy: 0.5625\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2243 - accuracy: 1.0000 - val_loss: 251.7555 - val_accuracy: 0.5625\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2240 - accuracy: 1.0000 - val_loss: 251.7549 - val_accuracy: 0.5625\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2238 - accuracy: 1.0000 - val_loss: 251.7542 - val_accuracy: 0.5625\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2235 - accuracy: 1.0000 - val_loss: 251.7536 - val_accuracy: 0.5625\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2232 - accuracy: 1.0000 - val_loss: 251.7529 - val_accuracy: 0.5625\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2229 - accuracy: 1.0000 - val_loss: 251.7522 - val_accuracy: 0.5625\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2226 - accuracy: 1.0000 - val_loss: 251.7516 - val_accuracy: 0.5625\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2223 - accuracy: 1.0000 - val_loss: 251.7509 - val_accuracy: 0.5625\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2221 - accuracy: 1.0000 - val_loss: 251.7503 - val_accuracy: 0.5625\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2218 - accuracy: 1.0000 - val_loss: 251.7496 - val_accuracy: 0.5625\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2215 - accuracy: 1.0000 - val_loss: 251.7489 - val_accuracy: 0.5625\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.2212 - accuracy: 1.0000 - val_loss: 251.7483 - val_accuracy: 0.5625\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2210 - accuracy: 1.0000 - val_loss: 251.7476 - val_accuracy: 0.5625\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2207 - accuracy: 1.0000 - val_loss: 251.7470 - val_accuracy: 0.5625\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2204 - accuracy: 1.0000 - val_loss: 251.7463 - val_accuracy: 0.5625\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.2201 - accuracy: 1.0000 - val_loss: 251.7457 - val_accuracy: 0.5625\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.2198 - accuracy: 1.0000 - val_loss: 251.7450 - val_accuracy: 0.5625\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.2196 - accuracy: 1.0000 - val_loss: 251.7444 - val_accuracy: 0.5625\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 188ms/step - loss: 0.2193 - accuracy: 1.0000 - val_loss: 251.7437 - val_accuracy: 0.5625\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2190 - accuracy: 1.0000 - val_loss: 251.7431 - val_accuracy: 0.5625\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.2187 - accuracy: 1.0000 - val_loss: 251.7424 - val_accuracy: 0.5625\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2185 - accuracy: 1.0000 - val_loss: 251.7418 - val_accuracy: 0.5625\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2182 - accuracy: 1.0000 - val_loss: 251.7411 - val_accuracy: 0.5625\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 251.7404 - val_accuracy: 0.5625\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2176 - accuracy: 1.0000 - val_loss: 251.7398 - val_accuracy: 0.5625\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2173 - accuracy: 1.0000 - val_loss: 251.7391 - val_accuracy: 0.5625\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.2171 - accuracy: 1.0000 - val_loss: 251.7385 - val_accuracy: 0.5625\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.2168 - accuracy: 1.0000 - val_loss: 251.7378 - val_accuracy: 0.5625\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2165 - accuracy: 1.0000 - val_loss: 251.7372 - val_accuracy: 0.5625\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 167ms/step - loss: 0.2163 - accuracy: 1.0000 - val_loss: 251.7365 - val_accuracy: 0.5625\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2160 - accuracy: 1.0000 - val_loss: 251.7359 - val_accuracy: 0.5625\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2157 - accuracy: 1.0000 - val_loss: 251.7352 - val_accuracy: 0.5625\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.2154 - accuracy: 1.0000 - val_loss: 251.7346 - val_accuracy: 0.5625\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2152 - accuracy: 1.0000 - val_loss: 251.7339 - val_accuracy: 0.5625\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2149 - accuracy: 1.0000 - val_loss: 251.7333 - val_accuracy: 0.5625\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2146 - accuracy: 1.0000 - val_loss: 251.7327 - val_accuracy: 0.5625\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2143 - accuracy: 1.0000 - val_loss: 251.7320 - val_accuracy: 0.5625\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 251.7313 - val_accuracy: 0.5625\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2138 - accuracy: 1.0000 - val_loss: 251.7307 - val_accuracy: 0.5625\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2135 - accuracy: 1.0000 - val_loss: 251.7301 - val_accuracy: 0.5625\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2133 - accuracy: 1.0000 - val_loss: 251.7294 - val_accuracy: 0.5625\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.2130 - accuracy: 1.0000 - val_loss: 251.7287 - val_accuracy: 0.5625\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2127 - accuracy: 1.0000 - val_loss: 251.7281 - val_accuracy: 0.5625\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2125 - accuracy: 1.0000 - val_loss: 251.7275 - val_accuracy: 0.5625\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2122 - accuracy: 1.0000 - val_loss: 251.7268 - val_accuracy: 0.5625\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2119 - accuracy: 1.0000 - val_loss: 251.7261 - val_accuracy: 0.5625\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.2117 - accuracy: 1.0000 - val_loss: 251.7255 - val_accuracy: 0.5625\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2114 - accuracy: 1.0000 - val_loss: 251.7249 - val_accuracy: 0.5625\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2111 - accuracy: 1.0000 - val_loss: 251.7242 - val_accuracy: 0.5625\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2109 - accuracy: 1.0000 - val_loss: 251.7236 - val_accuracy: 0.5625\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2106 - accuracy: 1.0000 - val_loss: 251.7229 - val_accuracy: 0.5625\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 251.7222 - val_accuracy: 0.5625\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.2101 - accuracy: 1.0000 - val_loss: 251.7216 - val_accuracy: 0.5625\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2098 - accuracy: 1.0000 - val_loss: 251.7210 - val_accuracy: 0.5625\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.2095 - accuracy: 1.0000 - val_loss: 251.7203 - val_accuracy: 0.5625\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2093 - accuracy: 1.0000 - val_loss: 251.7197 - val_accuracy: 0.5625\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.2090 - accuracy: 1.0000 - val_loss: 251.7190 - val_accuracy: 0.5625\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2087 - accuracy: 1.0000 - val_loss: 251.7184 - val_accuracy: 0.5625\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2085 - accuracy: 1.0000 - val_loss: 251.7177 - val_accuracy: 0.5625\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2082 - accuracy: 1.0000 - val_loss: 251.7171 - val_accuracy: 0.5625\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2079 - accuracy: 1.0000 - val_loss: 251.7164 - val_accuracy: 0.5625\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2077 - accuracy: 1.0000 - val_loss: 251.7158 - val_accuracy: 0.5625\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2074 - accuracy: 1.0000 - val_loss: 251.7152 - val_accuracy: 0.5625\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2072 - accuracy: 1.0000 - val_loss: 251.7145 - val_accuracy: 0.5625\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2069 - accuracy: 1.0000 - val_loss: 251.7139 - val_accuracy: 0.5625\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2066 - accuracy: 1.0000 - val_loss: 251.7132 - val_accuracy: 0.5625\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2064 - accuracy: 1.0000 - val_loss: 251.7126 - val_accuracy: 0.5625\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2061 - accuracy: 1.0000 - val_loss: 251.7120 - val_accuracy: 0.5625\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.2058 - accuracy: 1.0000 - val_loss: 251.7113 - val_accuracy: 0.5625\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.2056 - accuracy: 1.0000 - val_loss: 251.7107 - val_accuracy: 0.5625\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2053 - accuracy: 1.0000 - val_loss: 251.7100 - val_accuracy: 0.5625\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2051 - accuracy: 1.0000 - val_loss: 251.7094 - val_accuracy: 0.5625\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2048 - accuracy: 1.0000 - val_loss: 251.7087 - val_accuracy: 0.5625\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2046 - accuracy: 1.0000 - val_loss: 251.7081 - val_accuracy: 0.5625\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2043 - accuracy: 1.0000 - val_loss: 251.7075 - val_accuracy: 0.5625\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2040 - accuracy: 1.0000 - val_loss: 251.7068 - val_accuracy: 0.5625\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2038 - accuracy: 1.0000 - val_loss: 251.7062 - val_accuracy: 0.5625\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2035 - accuracy: 1.0000 - val_loss: 251.7055 - val_accuracy: 0.5625\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.2033 - accuracy: 1.0000 - val_loss: 251.7049 - val_accuracy: 0.5625\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2030 - accuracy: 1.0000 - val_loss: 251.7043 - val_accuracy: 0.5625\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2028 - accuracy: 1.0000 - val_loss: 251.7037 - val_accuracy: 0.5625\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2025 - accuracy: 1.0000 - val_loss: 251.7030 - val_accuracy: 0.5625\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2022 - accuracy: 1.0000 - val_loss: 251.7023 - val_accuracy: 0.5625\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.2020 - accuracy: 1.0000 - val_loss: 251.7017 - val_accuracy: 0.5625\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2017 - accuracy: 1.0000 - val_loss: 251.7011 - val_accuracy: 0.5625\n",
      "Epoch 447/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2015 - accuracy: 1.0000 - val_loss: 251.7004 - val_accuracy: 0.5625\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2012 - accuracy: 1.0000 - val_loss: 251.6998 - val_accuracy: 0.5625\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2010 - accuracy: 1.0000 - val_loss: 251.6991 - val_accuracy: 0.5625\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2007 - accuracy: 1.0000 - val_loss: 251.6985 - val_accuracy: 0.5625\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2005 - accuracy: 1.0000 - val_loss: 251.6979 - val_accuracy: 0.5625\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2002 - accuracy: 1.0000 - val_loss: 251.6973 - val_accuracy: 0.5625\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.2000 - accuracy: 1.0000 - val_loss: 251.6966 - val_accuracy: 0.5625\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.1997 - accuracy: 1.0000 - val_loss: 251.6960 - val_accuracy: 0.5625\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1995 - accuracy: 1.0000 - val_loss: 251.6954 - val_accuracy: 0.5625\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.1992 - accuracy: 1.0000 - val_loss: 251.6947 - val_accuracy: 0.5625\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.1990 - accuracy: 1.0000 - val_loss: 251.6940 - val_accuracy: 0.5625\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 185ms/step - loss: 0.1987 - accuracy: 1.0000 - val_loss: 251.6935 - val_accuracy: 0.5625\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 0.1985 - accuracy: 1.0000 - val_loss: 251.6928 - val_accuracy: 0.5625\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.1982 - accuracy: 1.0000 - val_loss: 251.6922 - val_accuracy: 0.5625\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.1980 - accuracy: 1.0000 - val_loss: 251.6915 - val_accuracy: 0.5625\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1977 - accuracy: 1.0000 - val_loss: 251.6909 - val_accuracy: 0.5625\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.1975 - accuracy: 1.0000 - val_loss: 251.6903 - val_accuracy: 0.5625\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.1972 - accuracy: 1.0000 - val_loss: 251.6897 - val_accuracy: 0.5625\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.1970 - accuracy: 1.0000 - val_loss: 251.6890 - val_accuracy: 0.5625\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.1967 - accuracy: 1.0000 - val_loss: 251.6884 - val_accuracy: 0.5625\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.1965 - accuracy: 1.0000 - val_loss: 251.6877 - val_accuracy: 0.5625\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.1962 - accuracy: 1.0000 - val_loss: 251.6871 - val_accuracy: 0.5625\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.1960 - accuracy: 1.0000 - val_loss: 251.6865 - val_accuracy: 0.5625\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.1957 - accuracy: 1.0000 - val_loss: 251.6858 - val_accuracy: 0.5625\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.1955 - accuracy: 1.0000 - val_loss: 251.6852 - val_accuracy: 0.5625\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1952 - accuracy: 1.0000 - val_loss: 251.6846 - val_accuracy: 0.5625\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.1950 - accuracy: 1.0000 - val_loss: 251.6839 - val_accuracy: 0.5625\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.1947 - accuracy: 1.0000 - val_loss: 251.6833 - val_accuracy: 0.5625\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.1945 - accuracy: 1.0000 - val_loss: 251.6827 - val_accuracy: 0.5625\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.1942 - accuracy: 1.0000 - val_loss: 251.6821 - val_accuracy: 0.5625\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.1940 - accuracy: 1.0000 - val_loss: 251.6814 - val_accuracy: 0.5625\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1938 - accuracy: 1.0000 - val_loss: 251.6808 - val_accuracy: 0.5625\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.1935 - accuracy: 1.0000 - val_loss: 251.6802 - val_accuracy: 0.5625\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.1933 - accuracy: 1.0000 - val_loss: 251.6796 - val_accuracy: 0.5625\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.1930 - accuracy: 1.0000 - val_loss: 251.6789 - val_accuracy: 0.5625\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1928 - accuracy: 1.0000 - val_loss: 251.6783 - val_accuracy: 0.5625\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1926 - accuracy: 1.0000 - val_loss: 251.6777 - val_accuracy: 0.5625\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.1923 - accuracy: 1.0000 - val_loss: 251.6770 - val_accuracy: 0.5625\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.1921 - accuracy: 1.0000 - val_loss: 251.6764 - val_accuracy: 0.5625\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 0.1918 - accuracy: 1.0000 - val_loss: 251.6758 - val_accuracy: 0.5625\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.1916 - accuracy: 1.0000 - val_loss: 251.6752 - val_accuracy: 0.5625\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.1913 - accuracy: 1.0000 - val_loss: 251.6745 - val_accuracy: 0.5625\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.1911 - accuracy: 1.0000 - val_loss: 251.6739 - val_accuracy: 0.5625\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.1909 - accuracy: 1.0000 - val_loss: 251.6733 - val_accuracy: 0.5625\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 0.1906 - accuracy: 1.0000 - val_loss: 251.6727 - val_accuracy: 0.5625\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.1904 - accuracy: 1.0000 - val_loss: 251.6720 - val_accuracy: 0.5625\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 248ms/step - loss: 0.1902 - accuracy: 1.0000 - val_loss: 251.6714 - val_accuracy: 0.5625\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.1899 - accuracy: 1.0000 - val_loss: 251.6707 - val_accuracy: 0.5625\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.1897 - accuracy: 1.0000 - val_loss: 251.6701 - val_accuracy: 0.5625\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.1894 - accuracy: 1.0000 - val_loss: 251.6695 - val_accuracy: 0.5625\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 222ms/step - loss: 0.1892 - accuracy: 1.0000 - val_loss: 251.6689 - val_accuracy: 0.5625\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1890 - accuracy: 1.0000 - val_loss: 251.6683 - val_accuracy: 0.5625\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.1887 - accuracy: 1.0000 - val_loss: 251.6676 - val_accuracy: 0.5625\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1885 - accuracy: 1.0000 - val_loss: 251.6670 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "history = model.fit(X, y,validation_split=0.33, epochs=500, batch_size=50\n",
    "\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b862d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step - loss: 84.6007 - accuracy: 0.8531\n",
      "Accuracy: 85.31\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001C1E419B790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001C1E419B790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001C1E419B790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "predicted value is[1, 1, 0, 1, 1]\n",
      "actual value was[1.0, 1.0, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\assets\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))\n",
    "\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(X2)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "\n",
    "print(\"predicted value is\"+str(rounded))\n",
    "print(\"actual value was\"+str(list(df.iloc[row_num_for_verification_of_model:,column_number_of_csv_having_labels])))\n",
    "\n",
    "model.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "678116a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48241</th>\n",
       "      <th>48242</th>\n",
       "      <th>48243</th>\n",
       "      <th>48244</th>\n",
       "      <th>48245</th>\n",
       "      <th>48246</th>\n",
       "      <th>48247</th>\n",
       "      <th>48248</th>\n",
       "      <th>48249</th>\n",
       "      <th>48250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-199.0</td>\n",
       "      <td>-185.0</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>273.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2147.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>2478.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>2113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6498.0</td>\n",
       "      <td>-9747.0</td>\n",
       "      <td>-11602.0</td>\n",
       "      <td>-9812.0</td>\n",
       "      <td>-2936.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>8695.0</td>\n",
       "      <td>17169.0</td>\n",
       "      <td>24004.0</td>\n",
       "      <td>26338.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1356.0</td>\n",
       "      <td>-1430.0</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-1792.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>-168.0</td>\n",
       "      <td>-466.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>-711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-201.0</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2763.0</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>2897.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2870.0</td>\n",
       "      <td>2815.0</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>2629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4901.0</td>\n",
       "      <td>-7652.0</td>\n",
       "      <td>-8792.0</td>\n",
       "      <td>-8146.0</td>\n",
       "      <td>-5328.0</td>\n",
       "      <td>-2711.0</td>\n",
       "      <td>-2202.0</td>\n",
       "      <td>-4287.0</td>\n",
       "      <td>-5865.0</td>\n",
       "      <td>-5212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>-2282.0</td>\n",
       "      <td>-2676.0</td>\n",
       "      <td>-2820.0</td>\n",
       "      <td>-3664.0</td>\n",
       "      <td>-2373.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>4309.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3059.0</td>\n",
       "      <td>-3010.0</td>\n",
       "      <td>-3407.0</td>\n",
       "      <td>-3881.0</td>\n",
       "      <td>-4114.0</td>\n",
       "      <td>-3687.0</td>\n",
       "      <td>-3567.0</td>\n",
       "      <td>-3514.0</td>\n",
       "      <td>-3572.0</td>\n",
       "      <td>-3497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7496.0</td>\n",
       "      <td>-12876.0</td>\n",
       "      <td>-16931.0</td>\n",
       "      <td>-18997.0</td>\n",
       "      <td>-18989.0</td>\n",
       "      <td>-17406.0</td>\n",
       "      <td>-15095.0</td>\n",
       "      <td>-12883.0</td>\n",
       "      <td>-11240.0</td>\n",
       "      <td>-10160.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 48251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2       3       4       5       6      7      8       9  \\\n",
       "0    1.0    0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0     0.0   \n",
       "1    0.0    9.0   154.0   374.0   346.0    36.0  -199.0 -185.0 -164.0  -175.0   \n",
       "2    0.0   11.0     9.0    11.0    12.0    10.0    11.0   10.0    7.0    13.0   \n",
       "3    0.0   18.0    24.0    24.0    18.0    29.0    29.0   22.0   29.0    19.0   \n",
       "4    0.0    3.0     5.0     1.0     6.0     6.0     4.0    7.0    5.0    10.0   \n",
       "..   ...    ...     ...     ...     ...     ...     ...    ...    ...     ...   \n",
       "143  1.0 -201.0  -183.0   -73.0    -7.0   124.0   202.0  221.0  167.0   132.0   \n",
       "144  1.0    0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0     0.0   \n",
       "145  0.0    0.0     0.0     0.0    -1.0     0.0     1.0    0.0    0.0     0.0   \n",
       "146  0.0 -175.0 -2282.0 -2676.0 -2820.0 -3664.0 -2373.0 -256.0  635.0  4309.0   \n",
       "147  1.0    0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0     0.0   \n",
       "\n",
       "     ...   48241    48242    48243    48244    48245    48246    48247  \\\n",
       "0    ...     0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1    ...   273.0    704.0   1257.0   2018.0   2147.0   2170.0   2478.0   \n",
       "2    ... -6498.0  -9747.0 -11602.0  -9812.0  -2936.0   2850.0   8695.0   \n",
       "3    ... -1356.0  -1430.0   2027.0     64.0  -1792.0    884.0   -168.0   \n",
       "4    ...     9.0     -5.0     -8.0      8.0     16.0     22.0     23.0   \n",
       "..   ...     ...      ...      ...      ...      ...      ...      ...   \n",
       "143  ...   214.0    291.0    357.0    420.0    493.0    569.0    583.0   \n",
       "144  ...  2763.0   2821.0   2867.0   2897.0   2909.0   2900.0   2870.0   \n",
       "145  ... -4901.0  -7652.0  -8792.0  -8146.0  -5328.0  -2711.0  -2202.0   \n",
       "146  ... -3059.0  -3010.0  -3407.0  -3881.0  -4114.0  -3687.0  -3567.0   \n",
       "147  ... -7496.0 -12876.0 -16931.0 -18997.0 -18989.0 -17406.0 -15095.0   \n",
       "\n",
       "       48248    48249    48250  \n",
       "0        0.0      0.0      0.0  \n",
       "1     2268.0   2221.0   2113.0  \n",
       "2    17169.0  24004.0  26338.0  \n",
       "3     -466.0    546.0   -711.0  \n",
       "4       -2.0    -23.0     -3.0  \n",
       "..       ...      ...      ...  \n",
       "143    484.0    334.0    237.0  \n",
       "144   2815.0   2735.0   2629.0  \n",
       "145  -4287.0  -5865.0  -5212.0  \n",
       "146  -3514.0  -3572.0  -3497.0  \n",
       "147 -12883.0 -11240.0 -10160.0  \n",
       "\n",
       "[148 rows x 48251 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c78b167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>48241</th>\n",
       "      <th>48242</th>\n",
       "      <th>48243</th>\n",
       "      <th>48244</th>\n",
       "      <th>48245</th>\n",
       "      <th>48246</th>\n",
       "      <th>48247</th>\n",
       "      <th>48248</th>\n",
       "      <th>48249</th>\n",
       "      <th>48250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-201.0</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2763.0</td>\n",
       "      <td>2821.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>2897.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2870.0</td>\n",
       "      <td>2815.0</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>2629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4901.0</td>\n",
       "      <td>-7652.0</td>\n",
       "      <td>-8792.0</td>\n",
       "      <td>-8146.0</td>\n",
       "      <td>-5328.0</td>\n",
       "      <td>-2711.0</td>\n",
       "      <td>-2202.0</td>\n",
       "      <td>-4287.0</td>\n",
       "      <td>-5865.0</td>\n",
       "      <td>-5212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-175.0</td>\n",
       "      <td>-2282.0</td>\n",
       "      <td>-2676.0</td>\n",
       "      <td>-2820.0</td>\n",
       "      <td>-3664.0</td>\n",
       "      <td>-2373.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>4309.0</td>\n",
       "      <td>7581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3059.0</td>\n",
       "      <td>-3010.0</td>\n",
       "      <td>-3407.0</td>\n",
       "      <td>-3881.0</td>\n",
       "      <td>-4114.0</td>\n",
       "      <td>-3687.0</td>\n",
       "      <td>-3567.0</td>\n",
       "      <td>-3514.0</td>\n",
       "      <td>-3572.0</td>\n",
       "      <td>-3497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7496.0</td>\n",
       "      <td>-12876.0</td>\n",
       "      <td>-16931.0</td>\n",
       "      <td>-18997.0</td>\n",
       "      <td>-18989.0</td>\n",
       "      <td>-17406.0</td>\n",
       "      <td>-15095.0</td>\n",
       "      <td>-12883.0</td>\n",
       "      <td>-11240.0</td>\n",
       "      <td>-10160.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1       2       3       4       5       6      7      8       9  \\\n",
       "143 -201.0  -183.0   -73.0    -7.0   124.0   202.0  221.0  167.0   132.0   \n",
       "144    0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0     0.0   \n",
       "145    0.0     0.0     0.0    -1.0     0.0     1.0    0.0    0.0     0.0   \n",
       "146 -175.0 -2282.0 -2676.0 -2820.0 -3664.0 -2373.0 -256.0  635.0  4309.0   \n",
       "147    0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0     0.0   \n",
       "\n",
       "         10  ...   48241    48242    48243    48244    48245    48246  \\\n",
       "143    66.0  ...   214.0    291.0    357.0    420.0    493.0    569.0   \n",
       "144     0.0  ...  2763.0   2821.0   2867.0   2897.0   2909.0   2900.0   \n",
       "145     0.0  ... -4901.0  -7652.0  -8792.0  -8146.0  -5328.0  -2711.0   \n",
       "146  7581.0  ... -3059.0  -3010.0  -3407.0  -3881.0  -4114.0  -3687.0   \n",
       "147     0.0  ... -7496.0 -12876.0 -16931.0 -18997.0 -18989.0 -17406.0   \n",
       "\n",
       "       48247    48248    48249    48250  \n",
       "143    583.0    484.0    334.0    237.0  \n",
       "144   2870.0   2815.0   2735.0   2629.0  \n",
       "145  -2202.0  -4287.0  -5865.0  -5212.0  \n",
       "146  -3567.0  -3514.0  -3572.0  -3497.0  \n",
       "147 -15095.0 -12883.0 -11240.0 -10160.0  \n",
       "\n",
       "[5 rows x 48250 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36adae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>48241</th>\n",
       "      <th>48242</th>\n",
       "      <th>48243</th>\n",
       "      <th>48244</th>\n",
       "      <th>48245</th>\n",
       "      <th>48246</th>\n",
       "      <th>48247</th>\n",
       "      <th>48248</th>\n",
       "      <th>48249</th>\n",
       "      <th>48250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-199.0</td>\n",
       "      <td>-185.0</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>273.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2147.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>2478.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>2113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6498.0</td>\n",
       "      <td>-9747.0</td>\n",
       "      <td>-11602.0</td>\n",
       "      <td>-9812.0</td>\n",
       "      <td>-2936.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>8695.0</td>\n",
       "      <td>17169.0</td>\n",
       "      <td>24004.0</td>\n",
       "      <td>26338.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1356.0</td>\n",
       "      <td>-1430.0</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-1792.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>-168.0</td>\n",
       "      <td>-466.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>-711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>24.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1631.0</td>\n",
       "      <td>-1835.0</td>\n",
       "      <td>-2084.0</td>\n",
       "      <td>-2337.0</td>\n",
       "      <td>-2493.0</td>\n",
       "      <td>-2581.0</td>\n",
       "      <td>-2565.0</td>\n",
       "      <td>-2616.0</td>\n",
       "      <td>-2721.0</td>\n",
       "      <td>-2919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>-938.0</td>\n",
       "      <td>-1237.0</td>\n",
       "      <td>-2004.0</td>\n",
       "      <td>-1692.0</td>\n",
       "      <td>-2498.0</td>\n",
       "      <td>-1274.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-44.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1748.0</td>\n",
       "      <td>-1698.0</td>\n",
       "      <td>-450.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-556.0</td>\n",
       "      <td>-489.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>679.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-631.0</td>\n",
       "      <td>-1252.0</td>\n",
       "      <td>-1302.0</td>\n",
       "      <td>-1080.0</td>\n",
       "      <td>-841.0</td>\n",
       "      <td>-632.0</td>\n",
       "      <td>-400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12270.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>-1668.0</td>\n",
       "      <td>-2847.0</td>\n",
       "      <td>-299.0</td>\n",
       "      <td>-1771.0</td>\n",
       "      <td>-7515.0</td>\n",
       "      <td>-10467.0</td>\n",
       "      <td>-6638.0</td>\n",
       "      <td>1703.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 48250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1       2      3      4       5       6       7      8      9     10  \\\n",
       "0      0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0    0.0    0.0   \n",
       "1      9.0   154.0  374.0  346.0    36.0  -199.0  -185.0 -164.0 -175.0 -175.0   \n",
       "2     11.0     9.0   11.0   12.0    10.0    11.0    10.0    7.0   13.0    5.0   \n",
       "3     18.0    24.0   24.0   18.0    29.0    29.0    22.0   29.0   19.0   20.0   \n",
       "4      3.0     5.0    1.0    6.0     6.0     4.0     7.0    5.0   10.0    5.0   \n",
       "..     ...     ...    ...    ...     ...     ...     ...    ...    ...    ...   \n",
       "138   24.0    66.0   65.0   62.0    53.0    37.0    30.0   65.0   68.0   69.0   \n",
       "139    0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0    0.0    0.0   \n",
       "140    0.0    -1.0   19.0   40.0    28.0    24.0    28.0  -26.0  -44.0  -31.0   \n",
       "141    0.0     0.0    0.0    0.0     0.0     0.0     0.0    0.0    0.0    0.0   \n",
       "142  679.0  1002.0   -4.0 -631.0 -1252.0 -1302.0 -1080.0 -841.0 -632.0 -400.0   \n",
       "\n",
       "     ...    48241   48242    48243   48244   48245   48246   48247    48248  \\\n",
       "0    ...      0.0     0.0      0.0     0.0     0.0     0.0     0.0      0.0   \n",
       "1    ...    273.0   704.0   1257.0  2018.0  2147.0  2170.0  2478.0   2268.0   \n",
       "2    ...  -6498.0 -9747.0 -11602.0 -9812.0 -2936.0  2850.0  8695.0  17169.0   \n",
       "3    ...  -1356.0 -1430.0   2027.0    64.0 -1792.0   884.0  -168.0   -466.0   \n",
       "4    ...      9.0    -5.0     -8.0     8.0    16.0    22.0    23.0     -2.0   \n",
       "..   ...      ...     ...      ...     ...     ...     ...     ...      ...   \n",
       "138  ...  -1631.0 -1835.0  -2084.0 -2337.0 -2493.0 -2581.0 -2565.0  -2616.0   \n",
       "139  ...   1083.0  1362.0    680.0  -938.0 -1237.0 -2004.0 -1692.0  -2498.0   \n",
       "140  ...    -78.0   -52.0     -6.0    58.0    52.0     6.0     8.0     -4.0   \n",
       "141  ...  -1748.0 -1698.0   -450.0   -53.0   165.0  -109.0  -556.0   -489.0   \n",
       "142  ...  12270.0  5800.0  -1668.0 -2847.0  -299.0 -1771.0 -7515.0 -10467.0   \n",
       "\n",
       "       48249    48250  \n",
       "0        0.0      0.0  \n",
       "1     2221.0   2113.0  \n",
       "2    24004.0  26338.0  \n",
       "3      546.0   -711.0  \n",
       "4      -23.0     -3.0  \n",
       "..       ...      ...  \n",
       "138  -2721.0  -2919.0  \n",
       "139  -1274.0    182.0  \n",
       "140    -15.0      6.0  \n",
       "141    540.0    639.0  \n",
       "142  -6638.0   1703.0  \n",
       "\n",
       "[143 rows x 48250 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
